# Laborat√≥rio Jogo da Vida Paralelo

## Alunos
| Nome | Matr√≠cula |
| ---- | --------- |
| Victor Eduardo Ara√∫jo Ribeiro | 190038926 |
| Pedro Victor Lima Torre√£o | 190036761 |

## üìã Vis√£o Geral
Este projeto implementa diferentes vers√µes paralelas do **Jogo da Vida de Conway** para compara√ß√£o de performance entre paradigmas de programa√ß√£o paralela:

- **MPI** (Message Passing Interface) - mem√≥ria distribu√≠da
- **OpenMP** - mem√≥ria compartilhada
- **CUDA** - computa√ß√£o em GPU NVIDIA
- **OpenMP GPU** - offloading para GPU com OpenMP

## üèóÔ∏è Estrutura do Projeto
```
jogo_da_vida/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ jogodavida.c          # Vers√£o sequencial original
‚îÇ   ‚îú‚îÄ‚îÄ jogodavidampi.c       # Vers√£o MPI
‚îÇ   ‚îú‚îÄ‚îÄ jogodavidaomp.c       # Vers√£o OpenMP
‚îÇ   ‚îú‚îÄ‚îÄ jogodavida.cu         # Vers√£o CUDA
‚îÇ   ‚îî‚îÄ‚îÄ jogodavidaomp_gpu.c   # Vers√£o OpenMP GPU
‚îú‚îÄ‚îÄ .gitignore                # Git Ignore 
‚îú‚îÄ‚îÄ Makefile                  # Makefile
‚îú‚îÄ‚îÄ README.md                 # Esta documenta√ß√£o
‚îî‚îÄ‚îÄ run_benchmark.sh          # Script de benchmark automatizado
```

## üîß Compila√ß√£o
### Pr√©-requisitos
- **GCC** 7.0+ com suporte OpenMP
- **Open MPI** ou Intel MPI
- **CUDA Toolkit** 10.0+ (para vers√£o CUDA)
- **OpenMP 4.5+** com suporte GPU (para vers√£o OpenMP GPU)


### Compilar Todas as Vers√µes
```bash
make all
```

### Compilar Vers√µes Espec√≠ficas
```bash
# Vers√£o sequencial
make jogodavida

# Vers√£o MPI
make jogodavidampi

# Vers√£o OpenMP
make jogodavidaomp

# Vers√£o CUDA
make jogodavida_cuda

# Vers√£o OpenMP GPU
make jogodavidaomp_gpu
```

## üöÄ Execu√ß√£o
### Testes B√°sicos
```bash
# Executar script benchmark
./run_benchmark.sh

# Executer Vers√£o sequencial
./exec/jogodavida

# Executer Vers√£o OpenMP
export OMP_NUM_THREADS=4
./exec/jogodavidaomp

# Executer Vers√£o MPI
mpirun -np 4 ./exec/jogodavidampi

# Executer Vers√£o CUDA
./exec/jogodavida_cuda

# Executer Vers√£o OpenMP GPU
export OMP_NUM_THREADS=4
./exec/jogodavidaomp_gpu
```

## üìä An√°lise de Resultados
### M√©tricas Coletadas

- **Tempo de inicializa√ß√£o** (`t_init`)
- **Tempo de computa√ß√£o** (`t_comp`) 
- **Tempo de finaliza√ß√£o** (`t_fim`)
- **Tempo total** (`t_total`)

### C√°lculo de Performance
```
Speedup = T_sequencial / T_paralelo
Efici√™ncia = Speedup / N√∫mero_de_Processadores
Throughput = C√©lulas_processadas / Tempo_computa√ß√£o
```

### Valida√ß√£o
Todas as vers√µes devem:
- ‚úÖ Produzir resultado "**RESULTADO CORRETO**"
- ‚úÖ Mover o veleiro do canto superior esquerdo ao inferior direito
- ‚úÖ Manter exatamente 5 c√©lulas vivas ao final

## üîç Detalhes das Implementa√ß√µes
### 1. Vers√£o MPI (`jogodavidampi.c`)
**Estrat√©gia**: Divis√£o horizontal do tabuleiro entre processos

**Caracter√≠sticas**:
- Processo 0 atua como coordenador
- Distribui√ß√£o equilibrada de linhas entre workers
- Comunica√ß√£o coletiva (`MPI_Bcast`, `MPI_Allgather`)
- Sincroniza√ß√£o a cada gera√ß√£o

**Comando de execu√ß√£o**:
```bash
mpirun -np <num_processos> ./jogodavidampi
```

### 2. Vers√£o OpenMP (`jogodavidaomp.c`)
**Estrat√©gia**: Paraleliza√ß√£o de loops com threads

**Caracter√≠sticas**:
- `#pragma omp parallel for` nos loops principais
- Scheduling est√°tico para balanceamento
- Redu√ß√£o paralela na verifica√ß√£o
- Inicializa√ß√£o paralela

**Vari√°veis de ambiente**:
```bash
export OMP_NUM_THREADS=<num_threads>
export OMP_SCHEDULE=static
```

### 3. Vers√£o CUDA (`jogodavida.cu`)
**Estrat√©gia**: Computa√ß√£o massivamente paralela em GPU

**Caracter√≠sticas**:
- Kernel otimizado com mem√≥ria compartilhada
- Grid 2D de blocos e threads
- Gest√£o expl√≠cita de mem√≥ria GPU
- Dois kernels: b√°sico e otimizado

**Configura√ß√£o de kernel**:
```cpp
dim3 blockSize(16, 16);                   // 256 threads por bloco
dim3 gridSize((tam+15)/16, (tam+15)/16);  // Cobertura do tabuleiro
```

### 4. Vers√£o OpenMP GPU (`jogodavidaomp_gpu.c`)
**Estrat√©gia**: Offloading para GPU usando diretivas OpenMP

**Caracter√≠sticas**:
- `#pragma omp target` para offloading
- Gest√£o autom√°tica e expl√≠cita de dados
- Fallback para CPU se GPU indispon√≠vel
- Teams e distribute para paraleliza√ß√£o GPU

**Diretivas principais**:
```cpp
#pragma omp target teams distribute parallel for collapse(2) \
        map(to: tabulIn[0:total_cells]) \
        map(from: tabulOut[0:total_cells])
```

## üìà Resultados Encontrados (m√©dia de 3 execu√ß√µes do benchmark)
### Performance T√≠pica

| Vers√£o8                | Tam=16     | Tam=32     | Tam=64      | Tam=128    | Tam=256    |
| -----                  | ------     | ------     | ------      | -------    | -------    |
| CUDA GPU               | 0.0607200s | 0.0003240s |  0.0006709s | 0.0012870s | 0.0028169s |
| MPI 1 processos        | 0.0000470s | 0.0000670s |  0.0002232s | 0.0014989s | 0.0106549s |
| MPI 2 processos        | 0.0000830s | 0.0000930s |  0.0003190s | 0.0016251s | 0.0086272s |
| MPI 4 processos        | 0.0001969s | 0.0002131s |  0.0004752s | 0.0031691s | 0.0163260s |
| MPI 8 processos        | N/A        | N/A        | N/A         | N/A        | N/A        |
| OpenMP 16 threads      | 0.0007749s | 0.0017190s |  0.0047998s | 0.0089328s | 0.0188398s |
| OpenMP 1 threads       | 0.0000169s | 0.0000319s |  0.0001781s | 0.0012751s | 0.0097780s |
| OpenMP 2 threads       | 0.0000129s | 0.0000370s |  0.0001328s | 0.0007319s | 0.0053899s |
| OpenMP 4 threads       | 0.0000191s | 0.0000470s |  0.0001302s | 0.0005140s | 0.0038860s |
| OpenMP 8 threads       | 0.0000329s | 0.0000870s |  0.0001740s | 0.0006409s | 0.0035090s |
| OpenMP-GPU GPU offload | 0.0039220s | 0.0085161s |  0.0194459s | 0.0459991s | 0.1004109s |
| Sequencial             | 0.0000160s | 0.0000198s |  0.0001540s | 0.0012200s | 0.0097461s |

### Conclus√£o
Este experimento comparou diferentes paradigmas de programa√ß√£o paralela aplicados ao Jogo da Vida de Conway, revelando insights importantes sobre a adequa√ß√£o de cada tecnologia para diferentes escalas do problema.

#### 1. An√°lise de Desempenho por Paradigma
**OpenMP (Mem√≥ria Compartilhada)**
- **Melhor caso**: OpenMP com 4 threads alcan√ßou speedup de at√© **3.15x** (tam=128: 0.0097461s ‚Üí 0.0038860s)
- **Escalabilidade**: Excelente para tamanhos m√©dios e grandes, com melhor efici√™ncia entre 2-8 threads
- **Overhead**: Para tamanhos pequenos (tam=8,16), o overhead de cria√ß√£o de threads supera os benef√≠cios

**CUDA (GPU)**
- **Melhor caso**: Speedup de **3.46x** para tam=128 (0.0097461s ‚Üí 0.0028169s)
- **Caracter√≠stica**: Alta lat√™ncia inicial (~0.06s) devido √† inicializa√ß√£o da GPU
- **Ponto de equil√≠brio**: S√≥ compensa para tam ‚â• 64, onde o paralelismo massivo supera o overhead

**MPI (Mem√≥ria Distribu√≠da)**
- **Melhor caso**: MPI com 2 processos alcan√ßou speedup de **1.13x** para tam=128
- **Limita√ß√£o**: Overhead de comunica√ß√£o entre processos limita ganhos significativos
- **Observa√ß√£o**: MPI com 8 processos falhou em todos os testes, indicando problemas de escalabilidade

**OpenMP GPU Offloading**
- **Desempenho**: Consistentemente pior que todas as outras vers√µes (10-50x mais lento)
- **Causa prov√°vel**: Overhead excessivo de transfer√™ncia de dados CPU‚ÜîGPU sem otimiza√ß√£o adequada
- **Recomenda√ß√£o**: Necessita otimiza√ß√µes espec√≠ficas ou n√£o √© adequado para este problema

#### 2. Percentuais de Ganho Comparativos
| Compara√ß√£o | Tam=64 | Tam=128 |
|------------|---------|----------|
| OpenMP 4 threads vs Sequencial | **137%** mais r√°pido | **151%** mais r√°pido |
| CUDA vs Sequencial | **-5%** mais devagar | **246%** mais r√°pido |
| CUDA vs OpenMP 4 threads | **-150%** mais devagar | **38%** mais r√°pido |
| MPI 2 proc vs Sequencial | **-33%** mais devagar | **13%** mais r√°pido |

#### 3. Recomenda√ß√µes por Tamanho de Problema
- **Tam ‚â§ 32**: Use a vers√£o **sequencial** ou **OpenMP com 2 threads**
- **Tam = 64**: Use **OpenMP com 4-8 threads** (melhor custo-benef√≠cio)
- **Tam ‚â• 128**: Use **CUDA** para m√°xima performance ou **OpenMP 8 threads** para boa performance sem GPU

#### 4. Achados Importantes
1. **Overhead vs Paralelismo**: O overhead de inicializa√ß√£o √© cr√≠tico para problemas pequenos. CUDA tem overhead de ~60ms, tornando-a invi√°vel para tam < 64.

2. **Efici√™ncia de Threads**: OpenMP mostra melhor efici√™ncia com 4-8 threads. Com 16 threads, a conten√ß√£o e sincroniza√ß√£o degradam a performance.

3. **Valida√ß√£o**: Todas as vers√µes implementadas (exceto MPI 8 processos) produziram resultados corretos, movendo o veleiro do canto superior esquerdo para o inferior direito.

4. **GPU Efficiency**: A implementa√ß√£o CUDA mostrou-se eficiente apenas para problemas grandes, sugerindo que o kernel poderia ser otimizado para melhor ocupa√ß√£o da GPU em problemas menores.

5. **Surpresa**: OpenMP superou CUDA em alguns casos m√©dios (tam=64), demonstrando que nem sempre GPU √© a melhor escolha, especialmente considerando o overhead de setup.

#### 5. Conclus√£o Final 
O experimento demonstra que **n√£o existe uma solu√ß√£o √∫nica melhor para todos os casos**. A escolha do paradigma deve considerar:
- **Tamanho do problema**: Fundamental para determinar se o overhead compensa
- **Hardware dispon√≠vel**: GPU necess√°ria para CUDA, m√∫ltiplos cores para OpenMP
- **Facilidade de implementa√ß√£o**: OpenMP oferece melhor rela√ß√£o facilidade/performance

Para o cluster utilizado no experimento, **OpenMP com 4-8 threads** apresentou o melhor equil√≠brio entre performance, facilidade de uso e consist√™ncia across diferentes tamanhos de problema.

## üìö Refer√™ncias
1. **Conway's Game of Life**: Gardner, M. "Mathematical Games", Scientific American 223, Oct 1970
2. **MPI Documentation**: [https://www.open-mpi.org/doc/](https://www.open-mpi.org/doc/)
3. **OpenMP Specification**: [https://www.openmp.org/specifications/](https://www.openmp.org/specifications/)
4. **CUDA Programming Guide**: [https://docs.nvidia.com/cuda/](https://docs.nvidia.com/cuda/)
5. **OpenMP GPU Offloading**: [https://www.openmp.org/updates/openmp-accelerator-support-gpus/](https://www.openmp.org/updates/openmp-accelerator-support-gpus/)
